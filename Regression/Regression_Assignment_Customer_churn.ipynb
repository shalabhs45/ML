{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shalabhs45/ML/blob/main/Regression/Regression_Assignment_Customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "U7CvrS-udnQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_curve"
      ],
      "metadata": {
        "id": "2SOi996ae3oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data from Google Drive"
      ],
      "metadata": {
        "id": "6Nvq_4ksdurK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HZ5rPUd7mpaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the provided Excel file into a Pandas DataFrame. Skip the first row and use the second row as the header.\n",
        "# TODO - write your solution here\n",
        "file_path = None\n",
        "data = pd.read_csv(file_path, header=0)"
      ],
      "metadata": {
        "id": "7G0ui3zDiQKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "rQwzSgkMk88p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please find dataset from the uplevel link\n",
        "\n",
        "Context\n",
        "\"Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\" [IBM Sample Data Sets]\n",
        "\n",
        "\n",
        "\n",
        "Content\n",
        "Each row represents a customer, each column contains customer’s attributes described on the column Metadata.\n",
        "\n",
        "\n",
        "\n",
        "The data set includes information about:\n",
        "\n",
        "Customers who left within the last month – the column is called Churn\n",
        "Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
        "Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
        "Demographic info about customers – gender, age range, and if they have partners and dependents"
      ],
      "metadata": {
        "id": "ldAamC_KlJmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove data columns\n",
        "data.drop(columns=['ID'], inplace=True)"
      ],
      "metadata": {
        "id": "jmyjIsY-quP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:3]"
      ],
      "metadata": {
        "id": "jf_K1tQAtqkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA\n"
      ],
      "metadata": {
        "id": "astc5cbYd4E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot to visualize the distribution of the target variable 'Churn' in the given `data` DataFrame\n",
        "# # TODO - write your solution here"
      ],
      "metadata": {
        "id": "z-0mx2U16sa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Analyze the churn rate by categorical features in the given `data` DataFrame. Create a bar plot for each categorical feature, showing the churn count for each category, with a figure size of (10, 4). Add a title \"Churn by {column}\" to each plot, where {column} is the name of the categorical feature."
      ],
      "metadata": {
        "id": "dwhh_VYK_cKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze churn rate by categorical features\n",
        "categorical_columns = [\n",
        "    'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n",
        "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
        "    'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
        "]\n",
        "\n",
        "for column in categorical_columns:\n",
        "    # # TODO - write your solution here"
      ],
      "metadata": {
        "id": "Q0HfkNPW6ukD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Analyze the churn rate by numeric features in the given `data` DataFrame. Create a histogram for each numeric feature, showing the churn count for each numeric value, with a figure size of (10, 4). Add a title \"Churn by {column}\" to each plot, where {column} is the name of the numeric feature. Also, include a kernel density estimate (kde) in the histograms."
      ],
      "metadata": {
        "id": "fM1piTMK_xcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze churn rate by numeric features\n",
        "numeric_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "for column in numeric_columns:\n",
        "    ## TODO - write your solution here\n"
      ],
      "metadata": {
        "id": "mzeC7hXL7KLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Analyze the correlation between numeric features in the given `data` DataFrame. Create a heatmap to visualize the correlation matrix of the numeric features with a figure size of (8, 6). Use the \"coolwarm\" color map and add a title \"Correlation Matrix\" to the plot."
      ],
      "metadata": {
        "id": "PyH52kuxAG78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the correlation between numeric features\n",
        "## TODO - write your solution here"
      ],
      "metadata": {
        "id": "24IzakVz7V-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M4GZZDCkAMDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the distribution of numeric features\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.pairplot(data[[*numeric_columns, 'Churn']], hue='Churn')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BPnaQyXS7ZUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "artMZVD_eCZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Convert the 'TotalCharges' column in the given `data` DataFrame to numeric values. Use the `pd.to_numeric` function and handle errors by setting them as 'coerce'. After converting the column, fill any missing values with the mean of the 'TotalCharges' column."
      ],
      "metadata": {
        "id": "vrnj2x8nAYIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert TotalCharges to numeric\n",
        "## TODO - write your solution here"
      ],
      "metadata": {
        "id": "nVud3wxytm9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Feature Handling"
      ],
      "metadata": {
        "id": "XQqMhGl2eSiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** One-hot encode the categorical features in the given `data` DataFrame. Use the `pd.get_dummies` function to create dummy variables for each categorical feature, and drop the first category for each feature."
      ],
      "metadata": {
        "id": "H_Ndd4XBAwJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode categorical features\n",
        "categorical_columns = [\n",
        "    'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n",
        "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
        "    'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
        "]\n",
        "data = ## TODO - write your solution here"
      ],
      "metadata": {
        "id": "QmnXaXkD-6Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the 'Churn' column\n",
        "data['Churn'] = ## TODO - write your solution here"
      ],
      "metadata": {
        "id": "gjbE0j_dx2cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "1Fcm7YZkecTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Split the given `data` DataFrame into training and testing sets. Drop the 'customerID' and 'Churn' columns from the input features (X), and use the 'Churn' column as the target variable (y). Use a test size of 0.2 and a random state of 42."
      ],
      "metadata": {
        "id": "qj95NgF-EA6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Write your solution here\n",
        "def split_data(data):\n",
        "    pass\n",
        "\n",
        "X_train, X_test, y_train, y_test = split_data(data)"
      ],
      "metadata": {
        "id": "8w0zZED_5imO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Scale the input features (X_train and X_test) using the StandardScaler from scikit-learn. Fit the scaler on the training data (X_train) and transform both the training and testing data."
      ],
      "metadata": {
        "id": "Mt2cmfO6BINw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "## TODO - write your solution here"
      ],
      "metadata": {
        "id": "YfEFEGMh5wtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Build a logistic regression model using scikit-learn's LogisticRegression class. Fit the model on the training data (X_train and y_train) and make predictions on the testing data (X_test)."
      ],
      "metadata": {
        "id": "mV4k0WAJBc1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "## TODO - write your solution here, use logisitc regression fit model\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ## TODO - write your solution here"
      ],
      "metadata": {
        "id": "H167-OtgxFMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Calculate and print the following evaluation metrics for the logistic regression model: accuracy, precision, recall, F1-score, and AUC (Area Under the Curve)."
      ],
      "metadata": {
        "id": "m4t6JvcUB39Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy, confusion matrix, and classification report\n",
        "accuracy = ## TODO - write your solution here,\n",
        "# Calculate precision, recall, F1-score, and AUC\n",
        "precision = ## TODO - write your solution here,\n",
        "recall = ## TODO - write your solution here,\n",
        "f1 = ## TODO - write your solution here,\n",
        "auc = ## TODO - write your solution here,\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-score: \", f1)\n",
        "print(\"AUC: \", auc)"
      ],
      "metadata": {
        "id": "aDI3uvQD7Crv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Calculate the probability of the target class (class 1) using the logistic regression model. Then, calculate the precision and recall values for different thresholds and plot the Precision-Recall curve with markers at each point."
      ],
      "metadata": {
        "id": "LxHsAGIYCQWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the probability of the target class (class 1)\n",
        "y_prob =  ## TODO - write your solution here,\n",
        "\n",
        "# Calculate precision and recall for different thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VSYZ8TI-jlS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyperParameter Tuning"
      ],
      "metadata": {
        "id": "TVTJ0xUiJNPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for parameter referes https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n"
      ],
      "metadata": {
        "id": "JrOyxn3Ws0bT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. `'C'`: This parameter represents the inverse of regularization strength. Smaller values of `C` result in stronger regularization, which can help prevent overfitting by penalizing large coefficients in the model.\n",
        "\n",
        "2. `'penalty'`: This parameter determines the type of regularization to be applied to the model. Regularization is used to prevent overfitting by penalizing large coefficients in the model. The `'l2'` penalty, also known as Ridge regularization, adds the squared magnitude of the coefficients to the loss function. In this case, you have specified only the `'l2'` penalty.\n",
        "\n",
        "3. `'solver'`: This parameter specifies the optimization algorithm to be used for training the logistic regression model. Different solvers have different performance characteristics and may be more suitable for specific types of datasets or problems. In this case, you have specified five solvers:\n",
        "\n",
        "   - `'newton-cg'`: Newton Conjugate Gradient, a second-order optimization method that approximates the Hessian matrix for efficient optimization. Suitable for large datasets and supports L2 regularization.\n",
        "   - `'lbfgs'`: Limited-memory Broyden-Fletcher-Goldfarb-Shanno, a quasi-Newton method that also approximates the Hessian matrix. Suitable for small to moderately-sized datasets and supports L2 regularization. It is the default solver in scikit-learn.\n",
        "   - `'liblinear'`: A linear solver that uses the coordinate gradient descent algorithm. Suitable for smaller datasets and supports both L1 and L2 regularization. However, it may have slower convergence for large datasets.\n",
        "   - `'sag'`: Stochastic Average Gradient descent, an optimization algorithm that uses a random sample of the data at each iteration. Suitable for large datasets and supports L2 regularization.\n",
        "   - `'saga'`: Stochastic Average Gradient descent with support for both L1 and L2 regularization. Suitable for large datasets and sparse data."
      ],
      "metadata": {
        "id": "03F_fVditOdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom parameter grid\n",
        "param_grid = [\n",
        "    {'penalty': ['l1'],\n",
        "     'solver': ['liblinear', 'saga'],\n",
        "     'C': [0.001, 0.01, 0.1, 1, 10],\n",
        "     'max_iter': [1000, 5000],\n",
        "     'class_weight': [None]\n",
        "    },\n",
        "    {'penalty': ['l2'],\n",
        "     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "     'C': [ 1, 10, 20, 50],\n",
        "     'max_iter': [1000, 5000],\n",
        "     'class_weight': [None]\n",
        "    },\n",
        "    {'penalty': ['elasticnet'],\n",
        "     'solver': ['saga'],\n",
        "     'C': [0.001, 0.01, 0.1, 1, 10, 50],\n",
        "     'max_iter': [1000, 5000],\n",
        "     'class_weight': [None],\n",
        "     'l1_ratio': [0, 0.3, 0.5, 0.7, 1]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "KRDCM2axC0B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Create a custom parameter grid for tuning a logistic regression model using GridSearchCV with 5-fold cross-validation, and optimize for accuracy. Fit the grid search on the training data (X_train and y_train) and print the best parameters and the corresponding score. Evaluate the best model on the test set (X_test) and calculate the accuracy, precision, recall, F1-score, and AUC."
      ],
      "metadata": {
        "id": "YJ1441jvC4k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and tune the logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "grid_search = ## TODO - write your solution here,\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the corresponding score\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Accuracy: \", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "OGe8LY5EC-SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy, confusion matrix, and classification report\n",
        "accuracy = accuracy_score(y_test, y_pred_best)\n",
        "# Calculate precision, recall, F1-score, and AUC\n",
        "precision = precision_score(y_test, y_pred_best)\n",
        "recall = recall_score(y_test, y_pred_best)\n",
        "f1 = f1_score(y_test, y_pred_best)\n",
        "auc = roc_auc_score(y_test, y_pred_best)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-score: \", f1)\n",
        "print(\"AUC: \", auc)"
      ],
      "metadata": {
        "id": "1Q-ayoRd4KJF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}