{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCow7BEs3J4+9Yq/RR8iLo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shalabhs45/ML/blob/main/UML2/UML2Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question1:**\n",
        "You are given a dataset containing information about customers in a mall, including their age, annual income, and spending score. The mall wants to identify different customer segments while maintaining the topological structure of the given data. Write a Python function that takes the dataset as input and performs the suitable clustering on it.\n",
        "\n",
        "\n",
        "\n",
        "Also, visualize the data and clusters for comprehensibility. Your function should return the cluster labels assigned to each customer. You may apply both BIRCH and SOM\n",
        "\n"
      ],
      "metadata": {
        "id": "cd5EUgUvZevu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZMJ4G6rZaXk",
        "outputId": "3370aa19-35e6-4168-a745-16a30d5f989a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "usup_data = \"/content/drive/MyDrive/DS-Assignment/USML/USML2/IK_UNSUP_DATA.csv\""
      ],
      "metadata": {
        "id": "TuckZNt6Z7_h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install minisom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAbYNDIybJaW",
        "outputId": "91f4515f-9958-44cf-c418-b31b6fe59515"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minisom\n",
            "  Downloading MiniSom-2.3.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: minisom\n",
            "  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minisom: filename=MiniSom-2.3.1-py3-none-any.whl size=10588 sha256=f494a161817358c0eb14a69c13d6a0be1383b03910faed01f9e17ae557593602\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/92/d2/33bbda5f86fd8830510b16aa98c8dd420129b5cb24248fd6db\n",
            "Successfully built minisom\n",
            "Installing collected packages: minisom\n",
            "Successfully installed minisom-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Birch\n",
        "from minisom import MiniSom"
      ],
      "metadata": {
        "id": "JM1HUa_paqBw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for Birch Clustering\n",
        "def perform_birch_clustering(data):\n",
        "  # Load the dataset from a pandas\n",
        "  df = pd.DataFrame(data)\n",
        "  # Preprocessing: Drop any missing values\n",
        "\n",
        "  df.dropna(inplace = True)\n",
        "\n",
        "  # Extract the features\n",
        "\n",
        "  X = df.values\n",
        "\n",
        "# Create a BIRCH clustering model\n",
        "  birch = Birch(threshold=0.5, branching_factor=50)\n",
        "\n",
        "# Fit the BIRCH model on the dataset\n",
        "  birch.fit(X)\n",
        "\n",
        "# Assign cluster labels to each data point\n",
        "  cluster_labels = birch.predict(X)\n",
        "\n",
        "  return cluster_labels\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "PDaN4xbjbcGR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform SOM Clustering\n",
        "\n",
        "def perform_som_clustering(data):\n",
        "  #Load the data from Pandas\n",
        "  df = pd.dataframe(data)\n",
        "  df.dropna(inplace=True)\n",
        "  # Extract the features\n",
        "  X = df.value_counts\n",
        "\n",
        "  #Create a SOM clustering Model\n",
        "  som = MiniSom(10, 10, X.shape[1], sigma = 1.0, learning_rate=0.5, random_seed=42)\n",
        "\n",
        "  # Initialize the SOM weights\n",
        "  som.random_weights_init(X)\n",
        "\n",
        "  # Train the SOM\n",
        "  som.train_random(X, 100)\n",
        "\n",
        "  # Assign cluster labels to each customer\n",
        "  cluster_labels = np.zeros(X.shape[0])\n",
        "  for i, x in enumerate(X):\n",
        "        bmu = som.winner(x)\n",
        "        cluster_labels[i] = bmu[0] * 10 + bmu[1]\n",
        "\n",
        "  return cluster_labels"
      ],
      "metadata": {
        "id": "r7iV9kDKc5A5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "customer_data = pd.read_csv(usup_data)\n",
        "\n",
        "# Perform BIRCH clustering\n",
        "birch_labels = perform_birch_clustering(customer_data)\n",
        "\n",
        "# Perform SOM clustering\n",
        "som_labels = perform_som_clustering(customer_data)\n",
        "\n",
        "# Plotting the results\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# BIRCH clustering plot\n",
        "axs[0].scatter(np.array(customer_data)[:, 1], np.array(customer_data)[:, 2], c=birch_labels)\n",
        "axs[0].set_xlabel('Annual Income')\n",
        "axs[0].set_ylabel('Spending Score')\n",
        "axs[0].set_title('BIRCH Clustering')\n",
        "\n",
        "# SOM clustering plot\n",
        "axs[1].scatter(np.array(customer_data)[:, 1], np.array(customer_data)[:, 2], c=som_labels)\n",
        "axs[1].set_xlabel('Annual Income')\n",
        "axs[1].set_ylabel('Spending Score')\n",
        "axs[1].set_title('SOM Clustering')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "2hS089uwgOqd",
        "outputId": "8aa1432c-ec18-4816-f1d4-351c5eefee90"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-9a97165bcac7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Perform SOM clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msom_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_som_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Plotting the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-c6b89b4b115a>\u001b[0m in \u001b[0;36mperform_som_clustering\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mperform_som_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#Load the data from Pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Extract the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'dataframe'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 2:\n",
        "Given the following data, apply LDA on the data to retrieve the topics of the data sentences."
      ],
      "metadata": {
        "id": "csmHYsrbi12X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data = [\"I want to watch a movie this weekend.\",\n",
        "\"I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.\",\n",
        "\"I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.\",\n",
        "\"Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been so long!\",\n",
        "\"This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.\",\n",
        "]"
      ],
      "metadata": {
        "id": "KUPsxKH6i_Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"I want to watch a movie this weekend.\", \"I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.\", \"I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.\", \"Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been so long!\", \"This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.\", ]"
      ],
      "metadata": {
        "id": "9j80EsNklO3c"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0vI2HsilcwZ",
        "outputId": "b3fa878f-6aa9-4d1a-bff4-72789eb13dd3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import os\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "metadata": {
        "id": "KKyjm-8Wi7Ay"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize regex tokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "# Vectorize document using TF-IDF\n",
        "tfidf = TfidfVectorizer(lowercase=True,\n",
        "                        stop_words='english',\n",
        "                        ngram_range = (1,1),\n",
        "                        tokenizer = tokenizer.tokenize)\n",
        "\n",
        "# Fit and Transform the documents\n",
        "train_data = tfidf.fit_transform(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueNIe3lmjqjW",
        "outputId": "e3a1160b-9e61-4098-e5a2-a3a451a09ba4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=LatentDirichletAllocation(n_components=5)\n",
        "lda_matrix = model.fit_transform(train_data)\n",
        "lda_components=model.components_\n",
        "\n",
        "# Print the topics with their terms\n",
        "terms = tfidf.get_feature_names_out()\n",
        "\n",
        "for index, component in enumerate(lda_components):\n",
        "    zipped = zip(terms, component)\n",
        "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n",
        "    top_terms_list=list(dict(top_terms_key).keys())\n",
        "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtVEx6lVl7lg",
        "outputId": "492d9853-72d6-4fc6-eb1e-6d00560b4396"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0:  ['good', 'movies', 's', 'movie', 'want', 'weekend', 'books']\n",
            "Topic 1:  ['chill', 'like', 'long', 'nice', 'paint', 'read', 'time']\n",
            "Topic 2:  ['good', 'movies', 's', 'movie', 'want', 'weekend', 'books']\n",
            "Topic 3:  ['books', 'biology', 'blueberry', 'brains', 'changer', 'dispenza', 'dr']\n",
            "Topic 4:  ['watch', 'movie', 'want', 'weekend', 'amazon', 'cricket', 'don']\n"
          ]
        }
      ]
    }
  ]
}